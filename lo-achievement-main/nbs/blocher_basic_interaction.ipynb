{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal CLAS UI Elements\n",
    "> Creating the minimal gradio app with state, chat, and chat download\n",
    "\n",
    "In this notebook, we create a minimized version of the current app to be deployed onto Huggingface Space, which has the following components: \n",
    "\n",
    "- Student interface: \n",
    "    A chat interface where students interact with the chatbot that has had the instructor-designed prompt given to it but they cannot see the prompt. At the end of the conversation, entire convo is made available as downloadable JSON such that students can download it and turn it in to Brightspace."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp BasicInteraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll start by loading our own libraries. Keep in mind that if you're on Colab, you need to replace \"token\" below with your GitHub token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code only if you're using Google Colab\n",
    "#! pip install pip install git+https://ghusername:<token>@github.com/vanderbilt-data-science/lo-achievement.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run this code only if you're local and developing\n",
    "import os, sys\n",
    "\n",
    "# get parent of current working directory\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "#append to path\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/lo-achievement/ai_classroom_suite/UIBaseComponents.py:354: GradioUnusedKwargWarning: You have unused kwarg parameters in Box, please remove them: {'scale': 1}\n",
      "  with gr.Box(elem_id=\"sources-container\", scale=1):\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os\n",
    "import gradio as gr\n",
    "from ai_classroom_suite.UIBaseComponents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Helpers and Functionality\n",
    "\n",
    "In the next section, we'll create some helper functions to make sure we're able to create the interface we want, and that it behaves nicely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Interface Chatbot Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_tutor_reply(chat_tutor): \n",
    "  chat_tutor.get_direct_tutor_reply()\n",
    "  return gr.update(value=\"\", interactive=True), chat_tutor.conversation_memory, chat_tutor\n",
    "\n",
    "def get_conversation_history(chat_tutor):\n",
    "    return chat_tutor.conversation_memory, chat_tutor\n",
    "\n",
    "def initialize_mdl_wrapper(chat_tutor, openai_auth=None, **mdl_chain_kwargs):\n",
    "  \n",
    "  if 'OPENAI_API_KEY' not in os.environ.keys() or not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "      #print('Nope')\n",
    "      gr.Error('This app will not run successfully without an OPENAI_API_KEY as an environment variable. Please contact your administrator \\\n",
    "                and tell them to set the OPENAI_API_KEY environment variable.')\n",
    "    \n",
    "  if 'SECRET_PROMPT' not in os.environ.keys() or not os.environ.get(\"SECRET_PROMPT\"):\n",
    "      #print('Nada')\n",
    "      gr.Warning('Your app currently does not have a System Message. Please contact your administrator \\\n",
    "                  and tell them to set the System Message.')\n",
    "        \n",
    "  chat_tutor.value = initialize_basic_model(chat_tutor.value, openai_auth=openai_auth, **mdl_chain_kwargs)\n",
    "\n",
    "  if chat_tutor.value.tutor_chain is None:\n",
    "     gr.Warning('Your model seems to have failed to initialize. Did you set the OPENAI_API_KEY environment variable?')\n",
    "\n",
    "  return chat_tutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The User Interface\n",
    "\n",
    "Below, we put all of this information together with the actual formatting of the user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note for Instructors ###\n",
    "\n",
    "You need to provide an OpenAI API Key as well as a Secret Prompt to initialize the model. \n",
    "\n",
    "- To set the API Key, in the hosted app on Huggingface Space, go to ``Settings -> Variables and Secrets -> Secrets``, then replace ``OPENAI_API_KEY`` value with your key.\n",
    "If you haven't created one already, visit [platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys) to sign up for an account and get your personal API key. \n",
    "\n",
    "- To set the Secret Prompt, in the hosted app on Huggingface Space, go to ``Settings -> Variables and Secrets -> Secrets``, then replace ``SECRET_PROMPT`` value with your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging purposes in the Jupyter notebook, you probably want to set these, otherwise things get annoying.\n",
    "#os.environ['OPENAI_API_KEY'] = ''\n",
    "#os.environ['SECRET_PROMPT'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "        \n",
    "### User Interfaces ###\n",
    "with gr.Blocks() as BasicInteractionDemo:\n",
    "    #initialize tutor (with state)\n",
    "    study_tutor = gr.State(SlightlyDelusionalTutor())\n",
    "\n",
    "    if 'SECRET_PROMPT' in os.environ:\n",
    "        study_tutor.value.set_system_message(os.environ.get(\"SECRET_PROMPT\"))\n",
    "\n",
    "    # Student chatbot interface\n",
    "    gr.Markdown(\"\"\"\n",
    "    ## Chat with the Model\n",
    "    Description here\n",
    "    \"\"\")\n",
    "    \n",
    "    \"\"\"\n",
    "    API Authentication functionality\n",
    "    Instead of ask students to provide key, the key is now provided by the instructor. \n",
    "    To permanently set the key, go to Settings -> Variables and secrets -> Secrets, \n",
    "    then replace OPENAI_API_KEY value with whatever openai key of the instructor.\n",
    "    \"\"\"\n",
    "    api_input = gr.Textbox(show_label=False, type=\"password\", value=os.environ.get(\"OPENAI_API_KEY\"), visible=False)\n",
    "\n",
    "    # The instructor will provide a secret prompt/persona to the tutor\n",
    "    instructor_prompt = gr.Textbox(label=\"Verify your prompt content\", value = os.environ.get(\"SECRET_PROMPT\"), visible=False)\n",
    "    \n",
    "    # Placeholders components\n",
    "    text_input_none = gr.Textbox(visible=False)\n",
    "    file_input_none = gr.File(visible=False)\n",
    "    instructor_input_none = gr.TextArea(visible=False)\n",
    "    learning_objectives_none = gr.Textbox(visible=False)\n",
    "\n",
    "    # Initialize situation with API key so no click is necessary\n",
    "    study_tutor = initialize_mdl_wrapper(study_tutor, api_input.value)\n",
    "\n",
    "    with gr.Row(equal_height=True):\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot()\n",
    "            with gr.Row():\n",
    "                user_chat_input = gr.Textbox(label=\"User input\", scale=9)\n",
    "                user_chat_submit = gr.Button(\"Ask/answer model\", scale=1)\n",
    "\n",
    "    # First add user's message to the conversation history\n",
    "    # Then get reply from the tutor and add that to the conversation history\n",
    "    user_chat_submit.click(\n",
    "        fn = add_user_message, inputs = [user_chat_input, study_tutor], outputs = [user_chat_input, chatbot, study_tutor], queue=False\n",
    "    ).then(\n",
    "        fn = get_tutor_reply, inputs = [study_tutor], outputs = [user_chat_input, chatbot, study_tutor], queue=True\n",
    "    )\n",
    "\n",
    "    user_chat_input.submit(\n",
    "        fn = add_user_message, inputs = [user_chat_input, study_tutor], outputs = [user_chat_input, chatbot, study_tutor], queue=False\n",
    "    ).then(\n",
    "        fn = get_tutor_reply, inputs = [study_tutor], outputs = [user_chat_input, chatbot, study_tutor], queue=True\n",
    "    )\n",
    "\n",
    "    # Download conversation history file\n",
    "    with gr.Box():\n",
    "        gr.Markdown(\"\"\"\n",
    "        ## Export Your Chat History\n",
    "        Export your chat history as a .json, .txt, or .csv file\n",
    "        \"\"\")\n",
    "        with gr.Row():\n",
    "            export_dialogue_button_json = gr.Button(\"JSON\")\n",
    "            export_dialogue_button_txt = gr.Button(\"TXT\")\n",
    "            export_dialogue_button_csv = gr.Button(\"CSV\")\n",
    "    \n",
    "        file_download = gr.Files(label=\"Download here\", file_types=['.json', '.txt', '.csv'], type=\"file\", visible=False)\n",
    "    \n",
    "    export_dialogue_button_json.click(save_json, study_tutor, file_download, show_progress=True)\n",
    "    export_dialogue_button_txt.click(save_txt, study_tutor, file_download, show_progress=True)\n",
    "    export_dialogue_button_csv.click(save_csv, study_tutor, file_download, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicInteractionDemo.queue().launch(server_name='0.0.0.0', server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little helper in case your ports are open and you just want to close them all. If this doesn't work, restart your IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Currently, the instructor prompt is handled as text input and stored in the vector store (and in the learning objective),\n",
    "# which means the tutor now is still a question-answering tutor who viewed the prompt as context (but not really acting based on it). \n",
    "# We need to find a way to provide the prompt directly to the model and set its status. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
